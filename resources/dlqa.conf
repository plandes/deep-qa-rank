#-*-conf-*-

[default]
resources_dir = ./resources
temporary_dir = ./target
results_dir = ./results
data_dir = ./data
proc_dir = ${data_dir}/proc
corpora_dir = ${data_dir}/corpora

[squad_1.1_corpus_parser]
class_name = SquadCorpusParser
version = 1.1
file_path = ${default:corpora_dir}/squad/{path_name}-v{version}.json
url = https://rajpurkar.github.io/SQuAD-explorer/dataset/{path_name}-v{version}.json

[train_corpus_reader]
class_name = SquadCorpusReader
dataset_name = train
path_name = train
corpus_parser = squad_1.1

[test_corpus_reader]
class_name = SquadCorpusReader
dataset_name = test
path_name = dev
corpus_parser = squad_1.1

[corpus]
all_readers = train,test
lang_parse_path = ${default:proc_dir}/lang_parsed
lang_feature_path = ${default:proc_dir}/lang_feature
ques_para_map_path = ${default:temporary_dir}/ques_para_map.dat
#features = i,children,dep,entity,is_punctuation,is_wh,shape,tag
#features = i,tag,ent,dep,entity,shape
features = i,tag,ent

[default_langres]
lang = en
model_name = ${lang}_core_web_sm
#model_name = ${lang}_core_web_lg

[lower_case_token_munger]
class_name = LambdaTokenMapper
map_lambda = lambda x: (x[0], x[1].lower())

[split_token_munger]
class_name = SplitTokenMapper
regex = r'[ _]'

[subs_token_munger]
class_name = SubstituteTokenMapper
regex = r'[ \t]'
replace_char = _

[filter_token_munger]
class_name = FilterTokenMapper
#remove_pronouns = True
#remove_stop = True

[corpus_token_normalizer]
class_name = MapTokenNormalizer
#embed_entities = False
normalize = False
#munger_class_list = eval: 'filter lower_case subs'.split()
munger_class_list = eval: 'filter subs'.split()

[gensim_goog_word_vector]
class_name = GensimWord2VecModel
path = ~/opt/var/model/word-vector/GoogleNews-vectors-negative300.bin
model_type = keyed
size = 300

[nn_model]
# the location of where the model is saved and loaded
model_path = ${default:temporary_dir}/doc_model.pt
# validation results output file
validation_path = ${default:results_dir}/validate.csv
# test results output file
test_path = ${default:results_dir}/test.txt
# predictions path
pred_path = ${default:results_dir}/pred.csv
# number of epochs to use for training
epochs = 20
# convolution filter/kernel width (height is the word vector dimension)
filter_width = 5
# convolution depth / number of filters
filter_depth = 70
# convolution stride
convolution_stride = 2
# max pool filter size
max_pool_fz = 4
# stride of the max pool layer
max_pool_stride = 4
# bilinear out
bilinear_out = 1
# gradient movement
learning_rate = 0.0004
# number of samples to load and train at a time
batch_size = 80
# percentage of training set to use as validation
valid_size = 0.2
# dropout (regularization)
dropout = 0.5
# whether or not to debug the network
debug = 0

[ranker]
results_path = ${default:temporary_dir}/rank

[lsa_model]
model_path = ${default:temporary_dir}/lsa_doc_model.dat
result_path = ${default:temporary_dir}/lsa_ranks.dat
# dimension of the SVD decomposition
n_vecs = 200
# K in the KNN classifier
n_neighbors = -1
#n_neighbors = 2
#n_neighbors = 20963
#n_neighbors = 18896
